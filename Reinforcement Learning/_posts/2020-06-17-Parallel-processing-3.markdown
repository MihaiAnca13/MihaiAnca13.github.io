---
layout: post
title:  "Parallel processing 3"
date:   2020-06-17 08:30:00 +0100
---
# Debugging
While debugging, I've realised that the 'done' flag was being set after only one call of the function 'step'. I've checked and the environment ends with a time limit after 50 actions taken. I've known that because I've reduced H to 10 so that the high level can take at least 5 actions before time's up. However, because H is 10 and the high level can take a maximum of 5 actions, the critic will never be able to predict the rewards. I'm surprised I've missed this before, because I had to add a special check for 'done' to be true after calling 'step' once. Another minnor bug I've found was that the steps taken was incremented both at low and high level. However, the low level should be the only one increasing this number. The maximum number of steps per episode can be changed like this:

~~~ python
env._max_episode_steps = 100
~~~

After this change, the returned reward after first episode is -100, as expected. However, the code now stops after processing first 2 episodes. This is because each process added one entry to the queue, making it full. The main loop was not processing the items in the queue because it was waiting for both low and high queues to have items on. I've added a timeout on the 'queue.get' of 0.1 seconds. However, if the timeout is reached, this raises an error. A try/except wrapper doesn't seem to fix the issue, because I get the following error:

~~~ python
Traceback (most recent call last):
  File "<string>", line 36, in <module>
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "<string>", line 36, in <module>
BrokenPipeError: [Errno 32] Broken pipe
~~~

The reason there are 2 pipes, is because I was afraid that the high level gradients would get lost among the low level ones, because of the difference in the number of transitions being processed for each level. However, now that I see that the queue awaits all items to be processed, I am confident it would work with one queue. I will revert the change, but track the training closely.

The training started eventually, however the graphs looks identical for the two processes. Moreover, rendering the test showed no change in the network. What's more weird, the target generated by the high level was not moving at all. There are a lot of bugs.

Another thing to keep in mind is the seed of each environment. Each should have a different one, however, the results should be easy to replicate. Here's the code for creating an env:

~~~ python
def make_env():
    global GLOBAL_SEED_COUNT
    env = gym.make(PARAMS['ENV_NAME'])
    env._max_episode_steps = 100
    env.seed(PARAMS['SEED']+GLOBAL_SEED_COUNT)
    GLOBAL_SEED_COUNT += 1
    return env
~~~

Currently, during training, there are two main issues:
- The high level generates almost identical goals
- The low level generates almost identical actions

I believe the main issue is with the low level. I need to identify what went wrong. It could be that the test is wrong, but the training is going fine. I will enable rendering while training. However, the low level loss is constantly increasing and the low level accuracy stays at 0. There's clearly something wrong.

After enabling rendering during training, I could see that the robot arm would move normally for half the episode, and then the high level would stop generating new goals. I've decided to try the code with 1 process and 1 environment. Same behaviour. However, this gave me the idea to look into the test_lambda behaviour. That was the problem! With test_lambda = 0, the arm movement is normal, therefore I need to find what went wrong in the agent.test function.