---
layout: post
title:  "Moving on"
date:   2020-09-15 08:30:00 +0100
---
<!-- ![Bug found](/assets/Common/bug-stop.png){: .center-image} -->
# What's next?
Given that the models have successfully learned the desired behaviour, it's time to move on. The achieved accuracy is not desirable and the training time is very slow. Therefore, to improve those, there are three changes to be made:
- force movements to be smooth by blending each position into the next one using an alpha hyperparameter
- using a prioritized replay buffer to "train more on data that surprises the model"
- add a demonstrations replay buffer to reduce the need for heavy exploration

## Smoothing
Using smoothing for position with the following formula:
~~~ python
target_gripper_pos = self.smoothness_alpha * target_gripper_pos + (1 - self.smoothness_alpha) * self.last_gripper_pos
self.last_gripper_pos = current_gripper_pos.copy()
~~~
Where smoothness_alpha was set to 0.3 for the following demonstration:

|   |   |
|:-:|:-:|
|![Gif](/assets/Moving-on/smooth.gif)|![Gif](/assets/Moving-on/jittery.gif)|

## Prioritized Experience Replay
References:
1. Schaul, T., Quan, J., Antonoglou, I. and Silver, D., 2015. Prioritized experience replay. arXiv preprint arXiv:1511.05952.
2. Horgan, D., Quan, J., Budden, D., Barth-Maron, G., Hessel, M., Van Hasselt, H. and Silver, D., 2018. Distributed prioritized experience replay. arXiv preprint arXiv:1803.00933.
3. Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., RothÃ¶rl, T., Lampe, T. and Riedmiller, M., 2017. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817.
4. Lapan, Maxim. Deep reinforcement learning hands-on : apply modern RL methods to practical problems of chatbots, robotics, discrete optimization, web automation, and more. Birmingham, UK: Packt Publishing Ltd, 2020. Print.

### Requirement list: 

Demonstration -> Prioritized Experience Replay -> Distributed Prioritized Experience Replay -> Data parallelism

Current parallelization architecture is gradient parallelism. This means that each process will no longer need to calculate and send gradients. However, the model parameters will periodically be updated. This requires yet another copy of the models. Moreover, the replay buffer doesn't need to be shared across processes. Critic model is used to calculate initial priorities for new samples. 

## Steps required for transition to data parallelism
1. Replay buffer no longer shared
2. Networks copy shared and updated periodically - every 400 frames
3. Remove queue
4. Move grad calculation to model.py
5. Sampling the buffer has to be done in parallel using a mp.Process and a queue of size 2. Therefore, each time a sample is consumed, another one is prepared in advance. Batch size is 256
6. Sending of data is done the same way, using a queue, that is emptied when filled. batch size = 50
7. Gradient clipping between -1 and 1

Params: 
- Prioritized replay buffer params: `alpha = 0.6, beta = 0.4`
- replay buffer size of 1 mil, allowing overflow, gets trimmed every 100 steps (using alpha evict of -0.4? - maybe not worth the hassle initially since comparison hasn't been studied in paper)


<!-- |  |   |   |   |   |
:-:|:-:|:-:|:-:|:-:|
![Low level accuracy](/assets/Getting-close/0_accuracy.png) | ![Low level actor loss](/assets/Getting-close/0_loss_actor.png) | ![Low level critic loss](/assets/Getting-close/0_loss_critic.png) | ![Low level reward](/assets/Getting-close/0_reward.png)
![High level accuracy](/assets/Getting-close/1_accuracy.png) | ![High level actor loss](/assets/Getting-close/1_loss_actor.png) | ![High level critic loss](/assets/Getting-close/1_loss_critic.png) | ![High level accuracy](/assets/Getting-close/1_reward.png)

![Gif](/assets/Getting-close/run0.gif) -->


<!-- ![Accuracy](/assets/Reduced-workspace-results/accuracy.png)
![Actor loss](/assets/Reduced-workspace-results/loss_actor.png)
![Critic loss](/assets/Reduced-workspace-results/loss_critic.png)

![Gif](/assets/Reduced-workspace-results/run0.gif) -->