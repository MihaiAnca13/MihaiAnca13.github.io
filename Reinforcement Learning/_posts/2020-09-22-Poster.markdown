---
layout: post
title:  "Poster"
date:   2020-09-22 08:30:00 +0100
---
<!-- ![Bug found](/assets/Common/bug-stop.png){: .center-image} -->
# Poster

Learning Long Chain of Actions Through Imitation:

![Poster](/assets/Poster/draft.png)

## Methodology/Description
The project focuses on applying Hierarchical Reinforcement Learning on the problem of sorting a pile of cluttered objects. This will have applications in nuclear decommissioning and in sorting recyclables. 

Reinforcement learning (RL) can be described as an agent following a policy Ï€ that select actions A based on states S sampled from the environment. Finding the best policy, that maximizes the reward in each episode can is achieved by satisfying the Bellman equation:

![Bellman](/assets/Poster/bellman.png)

The hypothesis of HRL is that large problems that can be decomposed hierarchically can be solved through a divide-and-conquer strategy, and, when put back together, they generate a more efficient solution. This is similar to a swarm's emergence, where the sum of parts is greater than the whole.

## Results/Conclusion + Graph
Training of both levels starts at the same time. While the low level manages to train very quickly (~400 ep), the high level can take 500 times longer. In order to check the effects of imperfect movements on accuracy, the low level was initialized with pre-trained weights or even replaced by hard-coded movements. Here are the best results obtained:

![Accuracy](/assets/High-accuracy/accuracy.png)

After re-enabling low level, and re-training the system, the accuracy obtained decreased to 37.5%. This result was slightly improved by smoothing the movements using an exponential mean: `target = alpha * target + (1 - alpha) * last`.

## High level description
The high level is described by the policy 

![Bellman](/assets/Poster/high_level.png)

that takes as input the full observed state S and the environment's goal G. The output actions are used as goals for the low level. In the experiments undertaken, for each target generated by the high level, the low level would be allowed to step 10 times through the environment.

## Problem complexity
Current experiments have been done on a personal implementation, in PyBullet, of the OpenAI's Pick and Place environment.

The complexity of the environment will be further increased by adding multiple objects.

Maximum complexity is represented by applying the algorithm on a real robot.

## Low level description
The low level is described by the policy

![Bellman](/assets/Poster/low_level.png)

that takes as input a reduced state, representing the position and state of the gripper. The high level action is used as goal here, while the actions generated are used to directly drive the robot.

## Robot/Environment description


## Demonstration


## Applying on real robot


## Theory / Novelty



<!-- |  |   |   |   |   |
:-:|:-:|:-:|:-:|:-:|
![Low level accuracy](/assets/Getting-close/0_accuracy.png) | ![Low level actor loss](/assets/Getting-close/0_loss_actor.png) | ![Low level critic loss](/assets/Getting-close/0_loss_critic.png) | ![Low level reward](/assets/Getting-close/0_reward.png)
![High level accuracy](/assets/Getting-close/1_accuracy.png) | ![High level actor loss](/assets/Getting-close/1_loss_actor.png) | ![High level critic loss](/assets/Getting-close/1_loss_critic.png) | ![High level accuracy](/assets/Getting-close/1_reward.png)

![Gif](/assets/Getting-close/run0.gif) -->


<!-- ![Accuracy](/assets/Reduced-workspace-results/accuracy.png)
![Actor loss](/assets/Reduced-workspace-results/loss_actor.png)
![Critic loss](/assets/Reduced-workspace-results/loss_critic.png)

![Gif](/assets/Reduced-workspace-results/run0.gif) -->