---
layout: post
title:  "Training 3"
date:   2020-07-08 08:30:00 +0100
---
# New approach needed
The training yesterday did not overfit, however, it did not learn to grasp the object either. 25.7k episodes per parallel process translates to approximately 205k episodes in total, which is more than used in the HER paper. 

I've noticed a bug in the loss checkpoint save mechanism. The last save happened at a loss of 0.14, however this value cannot be observed in Tensorboard. Downloading the data from tensor board in CSV format allowed the sorting of values. The actual lowest loss is 1.121. Looking at low level critic, the values are within this range (~0.12). The condition is clear:
~~~ python
if level > 0:
    return train_entry[level]['critic_loss']
~~~

This indicates there might be a bigger problem in the calculation and passing of gradients. Another difference found was that the polyak parameter was 0.999, while the paper uses 0.995. Gamma is currently set to 0.98, however in a past experience, setting it to 0.99 completely breaks the learning. Why is that?

First lead:
~~~ python
tracker.track(f"{n}_loss_critic", critic_loss_v, self.episode_nr)
~~~

This line is passing the tensor to the tracker instead of detaching, moving it to cpu and converting it to a numpy array.
~~~ python
tracker.track(f"{n}_loss_critic", critic_loss_v.detach().cpu().numpy(), self.episode_nr)
~~~

Unfortunately, this wasn't what's causing the issue. While debugging, one of those weird values was registered at episode 61. When checking tensorboard, this step has been skipped for both levels. This could be due to downscaling, but I find it weird that it's happening for all the values. By printing in the console, the values were 1.93, 1.79 at steps 216 and 217. However,on tensorboard, at the same steps, the values were 4.7 and 4.43. What is going on?

## Reasoning
Not all values are stored because for the same episode, there are 40 iterations. The loss results are all tracked under the same episode number. The tracker has been moved outside the `calculate_grads` function and now the proper value is reported. This now matches the loss at which the model is saved.

## Next problem
The episode steps graph shows that not all environments finish after 50 steps. Some end faster, reducing the average to sometimes 30. This is because `final_goal_during_low_level` allows the early stop of the low level. While debugging this, something else was observed: the `goal_transition` list for the high level contains 6 entries at the end of the loop. However, the maximum number of steps is 50.

### What happens when the object starts on the goal?
1. high level generates an action
2. low level makes one step then stops => experience added, no transition added
3. high level experience created and sample added to transition list
4. back to step 1. until 10 actions created

The experiences created for low level would say that regardless of the action taken, the output was negative since the action can almost never be reached in one step. There is one exception to this case, when the high level creates the action to be equal to the current position of the gripper. No transitions for low level from this interaction.

The failure of the low level to reach the action created by the high level creates an experience that would be used for high level model training. The final goal is reached, therefore a new experience with `gamma = 0` is created, regardless of the action proposed. Moreover, 10 transition samples are created which teach the same thing: random actions are rewarded since the object is already on the goal.

The low level is stopped when the robot reaches the final goal so that beneficial experiences are created for the high level. However, this specific case, which takes place quite commonly, does harm to the learning. This indicates that when the low level stops, a check must be put in place to identify these cases.

Since the transitions are duplicates of experiences in cases where the final goal is reached under the control of the low level, this means that no samples should be added as a transition when this is the case. What if no transitions are being added (ever) when the final goal is reached in the last sample considered? By deactivating this, the high level accuracy seems to stagnate. However, the low level accuracy was not affected. 