---
layout: post
title:  "Pre-trained Combined"
date:   2020-09-09 08:45:00 +0100
---
<!-- ![Bug found](/assets/Common/bug-stop.png){: .center-image} -->
# Debugging combined version
Given that there was a difference in training between high level only and the combined version, the next thing to try is to pre-train low level. This will re-create the same experiences for high level given that low level acts exactly as it should from the very beginning. If that is not the case, it means there is a bug and this should help finding it. In the first 4-5k episodes, the accuracy should start to increase a tiny bit.

Test is ongoing on reduced workspace, both levels enabled with low level pre-trained for 10k episodes.

### Note for readers
Last entry was updated with graphs of the long training. Accuracy of 92% achieved!

## Notes
The low level inefficiency in the beginning acts as an increased epsilon. Every action is random because the low level cannot achieve the target. This should not be an impediment to learning, but actually facilitate exploration early on.

## Test results
High level fails to produce that initial steady growth, even when low level behaves the same way as on high-only branch.

Possible areas where the bug could be:
- calculate grads
- apply grads
- transition creation
- step function
- parallel processing queue

## Freezing low level
In order to test whether the train queue affects the training of high level, the low level training has been frozen, therefore grads will only be calculated for high level. This turned out not to be the problem.

## Low level movement
Rather than removing low level, I will bypass it right before it steps through the environment. This turned out to be the problem. This means that the low level movements must have something to do with the ability of reaching the goal.

Discrepancy found: Bullet-HRL/Sim: height offset of the goal between 0.14 and 0.23 instead of 0.14 and 0.19. Because 0.19 is the upper most limit, that meant that 45% of sampled goals were outside of workspace. Unfortunately, this was not the problem.

Next, I replaced action with target directly on the develop-reduced branch. If it works, it means that the actual positions created by low level are the problem. How? No idea...

It works :(

## Analysing movement differences
The environment calls the simulator's step function 5 times every time an action is issued. If low level is used, the action is added to the current position and, using IK, the joint positions are generated. On the other hand, if low level is bypassed, the action is the target used in IK, therefore the joint positions would not change during those 5 steps. Moreover, the low level can take 10 actions before the target is changed. Therefore, a total of 50 simulator steps are undertaken for each target generated by the high level..

The question is, does the break down of the movement cause the issue, or is it the 10% difference in accuracy? It's almost like the low level introduces noise and the high level cannot learn to deal with that. In order to test this, I will bypass low level but add noise to the target generated. The noise amplitude is equal to maximum threshold, therefore accuracy is not affected. If this still works, the noise levels will be increased until accuracy equals 90%. This seems to be the cause.


<!-- ![Accuracy](/assets/High-accuracy/accuracy.png)
![Actor loss](/assets/High-accuracy/loss_actor.png)
![Critic loss](/assets/High-accuracy/loss_critic.png)

![Gif](/assets/High-accuracy/run0.gif) -->

<!-- ![Low level accuracy](/assets/Benefits-of-Normalization/0_accuracy.png)
![Low level actor loss](/assets/Benefits-of-Normalization/0_loss_actor.png)
![Low level critic loss](/assets/Benefits-of-Normalization/0_loss_critic.png)
![Low level reward](/assets/Normalization-3/0_reward.png)
![High level accuracy](/assets/Benefits-of-Normalization/1_accuracy.png)
![High level actor loss](/assets/Benefits-of-Normalization/1_loss_actor.png)
![High level critic loss](/assets/Benefits-of-Normalization/1_loss_critic.png)
![High level accuracy](/assets/Normalization-3/1_reward.png) -->