---
layout: post
title:  "Debugging Pick and Place + HAC"
date:   2020-06-10 08:18:00 +0100
---
# Environment
Yesterday, I've noticed the robot arm was moving outside the restricted workspace, therefore I'll start by investigating that. I plan on moving the red ball in the environment where the high level action is generated, rather than the ultimate goal. This can be accesed through the env.sim variable. The name of the red sphere in the environment is "target0". This has been found in the gym/envs/robotics/assets/fetch/pick\_and\_place.xml. The following code moves the sphere to the position generated:

~~~ python
site_id = self.env.sim.model.site_name2id('target0')
self.env.sim.model.site_pos[site_id] = target[:3]
~~~

Just like suspected, the target was being created way outside the bounded area. I will next look into the coordinates that I used to bound the state and I'll try to find the right values.

The mocap position is on a different coordinate system than the sphere. It's like they have different references. The z-axis is the same, however the x and y are offsetted. By looking into the pick\_and\_place.xml file, I've noticed the target sphere is generated as a child of the floor, which is positioned at 0.8, 0.75, 0. I've added those values to the position of the sphere generated by the sim.model and I've got the x, y, z of the sphere with reference to the world. The sim.data variable contains xpos, which is exactly what I described above with no need for further calculations.

Actually, the target is not created outside, I was moving the sphere using the wrong coordinates. After amending the code, it looks like this;

~~~ python
site_id = self.env.sim.model.site_name2id('target0')
self.env.sim.data.site_xpos[site_id] = target[:3]
~~~

However, it seems like you cannot change the values directly in data.site_xpos. The coordinates keep changing every loop getting smaller and smaller with each iteration. This is due to the \_render\_callback function inside the environment.

I've checked the environment code and it seems like the goal is represented in xpos. Moreover, the state boundaries are given w.r.t. the world. I believe the targets are properly generated, therefore, there's something else that's causing the arm to move outside the area. Here's an example of low level state and generated target:
1.34, 0.75, 0.54 compared to 1.5 0.74 0.83.

In order to investigate whether the low level can learn to follow targets, I've added a debug message whenever the low level goal is achieved. If during a new round of training I cannot see an increasing number of debug messages, that means my setup for the model/loss is somehow wrong. Also, I've managed to edit the xml file and add a new target1. I can now change the position of the intrinsic target using this newly created site:

~~~ python
sites_offset = (self.env.sim.data.site_xpos - self.env.sim.model.site_pos).copy()
site_id = self.env.sim.model.site_name2id('target1')
self.env.sim.model.site_pos[site_id] = target[:3] - sites_offset[site_id, :]
~~~

In over 200 episodes, the intrinsic target hasn't been achieved at least once. The low level it's clearly failing to learn. I've used the following code to track when the end-effector was getting really close to the target:

~~~ python
self.min_d = [1, 1, 1, 1]
a = low_level_next_state - target
if sum([abs(x) for x in a]) < sum([abs(x) for x in self.min_d]):
    self.min_d = a
    print(a)
~~~

I will begin by looking into the experiences added to the batch, followed by the loss calculation. I will also try to train the low level using different hyperparameters. OpenAI's baseline for DDPG+HER manages to learn the Fetch environment in under 5000 timesteps, which is the equivalent of the task for the low level model. I expect to be able to achieve similar performances once I fix the code.

With an initial batch size requirement of 1000 samples, it takes 20 episodes to gather enough data for training the low level. The error was in using the variable 'obs' instead of state['observation']. The 'obs' variable was set at the beginning of the step function, however, low level was being called in a for loop, with the state being overwritten. However, obs, was not being overwritten with the new state. Unfortunatelly, it seems like this is not the only issue.

I've also noticed the mean reward per episode is not always -50, which is weird considering the task never gets solved. The problem was resetting the done variable in the main loop when resetting the environment. Another issue I've noticed was the fact that the environment was returning a reward of 0, done equal to False and claiming that the object has reached the target. This was contradictory with my goal achieved function due to the difference in threshold set. Moreover, the env checks whether the object has reacher the target, while I was checking wheter the gripper has reached the target. This reward of 0 caused the experience buffer to fill with non-sense. After fixing all the problems, I've restarted the training. This time I could see some "High level success" messages. "Low level success" messages started to appear as well after 90 episodes, which is approximately 5000 steps.

Unfortunatelly, the low level is still not learning to follow the target. It's always moving the end-effector at the edge of it's workspace. I've also noticed that it usually takes 3 frames for the environment to set done to True. I don't know whether this is causing any issues. I'm forcing done to be set to True when the high level goal is achieved.

Before training, the arm does quite a good exploration. However, after first batch of training, the end-effector gets dragged at the end of the workspace. This might be because the arm doesn't succeed during first phases of exploration, or because something is wrong with the training.


While looking at the code and comparing it with the Hierarchical Actor Critic implementation, I've realised the experiences were wrongly created. There are 3 places where an experience gets created:
1. High level batch: created target was not reached
2. High level batch: each step, regardless of wheter the goal was reached or not
3. Low level batch: each step, regardless of wheter the target was reached or not

Fixing the code to follow the above guidelines took me one step closer. Another very important addition was the goal transition list. This list collects samples at each step. At the end of the loop, which means the episode is done or H steps have passed, the goal of each sample collected gets overwritten with the final state achieved. This helps the model predict H steps in advance what is going to happen and highly facilitates the learning.

### SUCCESS! The low level learns how to follow the target generated!

I am wondering whether it's beneficial turning off the low level learning once a high enough accuracy is achieved. If training continues, the model might overfit or weird behaviour might appear. This requires further testing.

