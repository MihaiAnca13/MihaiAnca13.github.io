---
layout: post
title:  "Expanding workspace"
date:   2020-09-03 08:30:00 +0100
---
<!-- ![Bug found](/assets/Common/bug-stop.png){: .center-image} -->
# Results high level only training
After training for 54k episodes p.p.p on the local machine with an increased learning rate:

![Accuracy](/assets/Expanding-workspace/accuracy.png)
![Actor loss](/assets/Expanding-workspace/loss_actor.png)
![Critic loss](/assets/Expanding-workspace/loss_critic.png)

<!-- ![Gif](/assets/Reduced-workspace-results/run0.gif) -->

## Changes on develop-bullet branch
The bug found yesterday must be fixed on the develop-bullet branch as well. This way, the supercomputer can use the latest updates to train models. This was achieved by cherry-picking the commits and merging the changes on both repositories. Given that the low level now manually controls the movement, the speed differs. Maybe it should be reduced, however, it managed to learn how to follow targets perfectly in the very beginning of the training.

High level, on the other hand, is creating very different points from what is was during high-only training. I'm hoping I didn't miss something while merging. This could also be due to the expansion of the workspace back to it's normal size. In each rendering, the arm moves away from the cube, instead of going towards it. It's still early to draw conclusions (4k ep).

After 7-8k I've decided to stop because it wasn't going anywhere. It was behaving like it learned early on not to touch the box. I've added a condition for the low level accuracy to be above 50% before punishing high level for out-of-range targets.

Re-enabling the debug message "Punished!" shows that even after achieving high accuracy in low level, the high level gets constantly punished for "out of reach" targets. This needs to be investigated.

![Bug found](/assets/Common/bug-stop.png){: .center-image}

The simulator clamps the gripper values between 0.01 and 0.04. When the high level generates a target with gripper state = 0, the difference will be at least 0.1. This values also equal the allowed threshold for gripper state. Therefore, in many cases, the low level achieves the target, yet the high level gets punished. Because the state space in which targets are generated by the high level is restricted, there are no target values that would be outside the work range of the robot arm. Therefore, I will completely disable this mechanism of "punishment". The question is whether setting testing probability to 0 will have an effect on the learning. This could be the case because, currently, more than 30% of actions are generated with no additional noise. Or, it could be that the additional noise will lead to better exploration and faster convergence.

With test_lambda = 0 and random_eps = 0.3, 30% of all actions taken are completely random. This will have a significant effect on the accuracy. With previous values, every action taken had 30% chance to constrain all further actions in that episode to have no noise. This means that for H=10, there is only 2.8% chance that all actions taken will have noise. 30% of those actions would be completely random. That's a huge difference. In conclusion, if setting test_lambda to 0, the random_eps must be significantly reduced. 
 
<!-- ![Low level accuracy](/assets/Benefits-of-Normalization/0_accurac.png)
![Low level actor loss](/assets/Benefits-of-Normalization/0_loss_actor.png)
![Low level critic loss](/assets/Benefits-of-Normalization/0_loss_critic.png)
![Low level reward](/assets/Normalization-3/0_reward.png)
![High level accuracy](/assets/Benefits-of-Normalization/1_accuracy.png)
![High level actor loss](/assets/Benefits-of-Normalization/1_loss_actor.png)
![High level critic loss](/assets/Benefits-of-Normalization/1_loss_critic.png)
![High level accuracy](/assets/Normalization-3/1_reward.png) -->