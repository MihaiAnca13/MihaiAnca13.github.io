<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2025-07-15T20:01:01+02:00</updated><id>/feed.xml</id><title type="html">Mihai Anca, PhD</title><subtitle>Personal website of Mihai Anca, Ph.D.</subtitle><author><name>Mihai Anca</name></author><entry><title type="html">Modular Hierarchical Reinforcement Learning for Robotics</title><link href="/2023-06-20-modular-hrl-for-robotics/" rel="alternate" type="text/html" title="Modular Hierarchical Reinforcement Learning for Robotics" /><published>2023-06-20T00:00:00+02:00</published><updated>2023-06-20T00:00:00+02:00</updated><id>/modular-hrl-for-robotics</id><content type="html" xml:base="/2023-06-20-modular-hrl-for-robotics/"><![CDATA[<h1 id="international-conference-on-machine-learning-icml-2023---frontiers4lcd-workshop">International Conference on Machine Learning (ICML) 2023 - Frontiers4LCD workshop</h1>

<h3 id="link---video"><a href="https://openreview.net/pdf?id=KIF8sUbr2k">Link</a> - <a href="https://www.veed.io/view/0d135379-6a6d-4241-bb32-76d26b41adc8?panel=share">Video</a></h3>

<h3 id="abstract">Abstract</h3>

<p>We present a novel software architecture for reinforcement learning applied to robotics that emphasizes modularity and reusability. Our method treats each agent as a plug-and-play ROS node that can be easily integrated into a larger HRL system, similar to using software libraries in programming. This modular approach improves the scalability and generalizability of pre-trained reinforcement learning agents. We demonstrate the effectiveness of our method by solving the real-world task of stacking three objects with two different robots that were trained only in simulation. Our results show that the modular approach significantly reduces the training and setup time required compared to a vanilla reinforcement learning baseline. Overall, our work showcases the potential of using trained agents as modules to enable the development of more complex and adaptable robotics applications.</p>

<p><img src="/assets/2023-06-20-modular-hrl-for-robotics/ur5e-setup-xs(1).jpg" alt="ur5e-setup-xs(1).jpg" /></p>

<p><img src="/assets/2023-06-20-modular-hrl-for-robotics/robot-setup-xs.png" alt="robot-setup-xs.png" /></p>]]></content><author><name>Mihai Anca</name></author><summary type="html"><![CDATA[International Conference on Machine Learning (ICML) 2023 - Frontiers4LCD workshop]]></summary></entry><entry><title type="html">Achieving Goals using Reward Shaping and Curriculum Learning</title><link href="/2022-02-24-achieving-goals/" rel="alternate" type="text/html" title="Achieving Goals using Reward Shaping and Curriculum Learning" /><published>2022-02-24T00:00:00+01:00</published><updated>2022-02-24T00:00:00+01:00</updated><id>/achieving-goals</id><content type="html" xml:base="/2022-02-24-achieving-goals/"><![CDATA[<h1 id="future-technology-conference-ftc-2023">Future Technology Conference (FTC) 2023</h1>

<h3 id="arxiv-link">Arxiv: <a href="https://arxiv.org/abs/2206.02462">Link</a></h3>

<h3 id="abstract">Abstract</h3>

<p>Real-time control for robotics is a popular research area in the reinforcement learning community. Through the use of techniques such as reward shaping, researchers have managed to train online agents across a multitude of domains. Despite these advances, solving goal-oriented tasks still requires complex architectural changes or hard constraints to be placed on the problem. In this article, we solve the problem of stacking multiple cubes by combining curriculum learning, reward shaping, and a high number of efficiently parallelized environments. We introduce two curriculum learning settings that allow us to separate the complex task into sequential sub-goals, hence enabling the learning of a problem that may otherwise be too difficult. We focus on discussing the challenges encountered while implementing them in a goal-conditioned environment. Finally, we extend the best configuration identified on a higher complexity environment with differently shaped objects.</p>

<p><img src="/assets/2022-02-24-achieving-goals/franka_sim_anyR.png" alt="franka_sim_anyR.png" /></p>

<p><img src="/assets/2022-02-24-achieving-goals/tetris_env(1).png" alt="tetris_env(1).png" /></p>]]></content><author><name>Mihai Anca</name></author><summary type="html"><![CDATA[Future Technology Conference (FTC) 2023]]></summary></entry><entry><title type="html">Multi-lingual agents through multi-headed neural networks</title><link href="/2021-11-22-multi-lingual-agents/" rel="alternate" type="text/html" title="Multi-lingual agents through multi-headed neural networks" /><published>2021-11-22T00:00:00+01:00</published><updated>2021-11-22T00:00:00+01:00</updated><id>/multi-lingual-agents</id><content type="html" xml:base="/2021-11-22-multi-lingual-agents/"><![CDATA[<h1 id="35th-conference-on-neural-information-processing-systems-neurips-2021---cooperative-ai-workshop">35th Conference on Neural Information Processing Systems (NeurIPS 2021) - Cooperative AI workshop</h1>

<h3 id="link"><a href="https://arxiv.org/abs/2111.11129">Link</a></h3>

<h3 id="abstract">Abstract</h3>

<p>This paper considers cooperative Multi-Agent Reinforcement Learning, focusing on emergent communication in settings where multiple pairs of independent learners interact at varying frequencies. In this context, multiple distinct and incompatible languages can emerge. When an agent encounters a speaker of an alternative language, there is a requirement for a period of adaptation before they can efficiently converse. This adaptation results in the emergence of a new language and the forgetting of the previous language. In principle, this is an example of the Catastrophic Forgetting problem which can be mitigated by enabling the agents to learn and maintain multiple languages. We take inspiration from the Continual Learning literature and equip our agents with multi-headed neural networks which enable our agents to be multi-lingual. Our method is empirically validated within a referential MNIST based communication game and is shown to be able to maintain multiple languages where existing approaches cannot.</p>

<p><img src="/assets/2021-11-22-multi-lingual-agents/mnist_fig.png" alt="mnist_fig.png" /></p>

<p><img src="/assets/2021-11-22-multi-lingual-agents/agent_rotate.png" alt="agent_rotate.png" /></p>]]></content><author><name>Mihai Anca</name></author><summary type="html"><![CDATA[35th Conference on Neural Information Processing Systems (NeurIPS 2021) - Cooperative AI workshop]]></summary></entry><entry><title type="html">Twin Delayed Hierarchical Actor-Critic</title><link href="/2021-02-04-twin-delayed-hierarchical-actor-critic/" rel="alternate" type="text/html" title="Twin Delayed Hierarchical Actor-Critic" /><published>2021-02-04T00:00:00+01:00</published><updated>2021-02-04T00:00:00+01:00</updated><id>/twin-delayed-hierarchical-actor-critic</id><content type="html" xml:base="/2021-02-04-twin-delayed-hierarchical-actor-critic/"><![CDATA[<h1 id="2021-7th-international-conference-on-automation-robotics-and-applications-icara">2021 7th International Conference on Automation, Robotics and Applications (ICARA)</h1>

<h3 id="link"><a href="https://ieeexplore.ieee.org/abstract/document/9376459">Link</a></h3>

<h3 id="abstract">Abstract</h3>

<p>Hierarchical Reinforcement Learning (HRL) addresses the common problem in sparse rewards environments of having to manually craft a reward function. We present a modified version of the Hierarchical Actor-Critic (HAC) architecture called Twin Delayed HAC (TDHAC), a method capable of sample-efficient learning on environments requiring object interaction. The vanilla algorithm fails to converge on this type of environment, while our method matches the best results so far reported in the literature. We carefully consider each feature added to the original architecture and demonstrate the abilities of TDHAC on the sparse-reward Pick-and-Place environment. To the best of our knowledge, this is the first HRL algorithm successfully applied on an environment requiring object interaction without external enhancements such as demonstrations.</p>

<p><img src="/assets/2021-02-04-twin-delayed-hierarchical-actor-critic/env-example1(1).png" alt="env-example1(1).png" /></p>]]></content><author><name>Mihai Anca</name></author><summary type="html"><![CDATA[2021 7th International Conference on Automation, Robotics and Applications (ICARA)]]></summary></entry></feed>